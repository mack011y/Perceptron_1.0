# Перцептрон для Бинарной Классификации

Реализация обучения однослойного перцептрона для задачи бинарной классификации с использованием только базовых математических операций и языка Go.

## Цель проекта

- Реализовать все вычисления через базовые операции
- Достичь точности не менее 85% на тестовой выборке
- Обеспечить работу с большими данными (1 млн примеров)
- Использовать параллельные вычисления для ускорения работы

## Особенности реализации

### Собственные математические функции

В проекте реализованы собственные версии основных математических функций для большей эффективности и точности:

- **Сигмоида**: оптимизированная реализация с использованием кусочной аппроксимации для разных диапазонов `z`:
  ```
  σ(z) ≈ {
    1/(1 + (1 - z + z²/2))             при z ≥ 0
    (1 + z + z²/2)/(1 + z + z²/2 + z³/6)  при z < 0
  }
  ```

- **Логарифм**: аппроксимация с использованием комбинированного подхода для улучшения точности при сохранении скорости вычислений

- **Экспонента**: эффективная аппроксимация с контролем ошибки

Каждая функция проходит сравнительное тестирование со стандартными математическими функциями Go, обеспечивая точность с малым отклонением (≤ 10⁻¹⁰).

### Генерация и подготовка данных

Проект включает генерацию синтетических данных для тестирования модели:

1. **Генерация данных**:
   - Создание синтетического набора данных по гипер параметрам
   - Управление размером выборки, количеством признаков и соотношением классов

2. **Разделение данных**:
   - Настраиваемое разделение на обучающую и тестовую выборки
   - Возможность перемешивания с фиксированным seed
   - Опциональная стратификация для сохранения распределения классов

## Модель перцептрона

1. **Линейная комбинация входных признаков**:
   ```
   z = Xw + b
   ```

2. **Сигмоидная функция для прогнозирования вероятности**:
   ```
   ŷ = σ(z) = 1 / (1 + e^(-z))
   ```

3. **Функция потерь (кросс-энтропия)**:
   ```
   J = -(1/m) ∑[y_i log(ŷ_i) + (1-y_i)log(1-ŷ_i)]
   ```

4. **Градиентный спуск для обновления весов**:
   ```
   ∂J/∂w_j = (1/m) ∑(ŷ_i - y_i)x_ij
   ∂J/∂b = (1/m) ∑(ŷ_i - y_i)
   ```

## Оценка модели

Для оценки производительности модели используются различные метрики:

- **Accuracy**: общая точность классификации
- **Precision**: точность положительных предсказаний
- **Recall**: полнота положительных предсказаний
- **F1-Score**: среднее гармоническое precision и recall
- **Confusion Matrix**: матрица ошибок для анализа предсказаний

## Зависимости

Проект имеет минимальные зависимости, используя только стандартную библиотеку Go:

- Go версии 1.18 или выше
- Стандартные пакеты:
  - `math`, `math/rand`
  - `time`
  - `fmt`

## Установка и запуск

### Клонирование репозитория

```bash
git clone https://github.com/username/perceptron.git
cd perceptron
```

### Запуск проекта

```bash
cd Perceptron
go run main.go
```

При запуске программа выполнит следующие шаги:
1. Тестирование точности реализации сигмоидной функции
2. Генерация и разделение синтетических данных
3. Обучение перцептрона с выводом прогресса по эпохам
4. Оценка модели и вывод метрик производительности

### Настройка гиперпараметров

Основные гиперпараметры можно настроить в файле `main.go`:
- Размер выборки и количество признаков
- Коэффициент обучения
- Количество эпох
- Соотношение обучающей и тестовой выборок
- Seed для воспроизводимости

## Структура проекта

- `main.go` - Точка входа в программу
- `model/` - Модель перцептрона и обучение
  - `perceptron.go` - Структура и методы перцептрона
  - `math.go` - Собственные реализации математических функций
  - `training.go` - Алгоритмы обучения
- `data/` - Работа с данными
  - `dataset.go` - Операции с наборами данных
  - `generator.go` - Генерация синтетических данных
- `utils/` - Вспомогательные функции
  - `metrics.go` - Функции для оценки производительности модели

## Авторы

Проект разработан как учебный, в ходе которого реализован простейщий перцептрон с одним слоем активации, без использования специализированных библиотек. 
